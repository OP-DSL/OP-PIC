{% extends "cpp/loop_host.hpp.jinja" %}

{% block kernel %}

namespace opp_k{{kernel_idx}} {
{{kernel_func}}

{% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
//--------------------------------------------------------------
__global__ void assign_values( // Used for Segmented Reductions
    const OPP_INT *__restrict keys,
    const OPP_REAL *__restrict values,
    OPP_REAL *__restrict dat,
    const int start,
    const int end) 
{
    const int tid = threadIdx.x + blockIdx.x * blockDim.x;

    if (tid + start < end) 
    {
        const int n = tid + start;
        const int mapping = keys[n];  
        dat[mapping] += values[n];
    }
}
{% endif %}
}

{% endblock %}

{% macro stride_cuda(arg) -%}
{{-" * opp_k%s_dat%s_stride_d" % (kernel_idx, arg.dat_id) if lh.dat(arg) is soa-}}
{%- endmacro %}

{% macro opt_cond(arg) %}
    {%- if arg is opt -%}arg{{arg.id}}.opt{%- endif -%}
{% endmacro %}

{% macro opt_cond_comp(arg) %}
    {%- if arg is opt -%}{{opt_cond(arg)}} && {% endif -%}
{% endmacro %}

{% macro opt_tern(arg, alt = "NULL") %}
    {%- if arg is opt -%}{{opt_cond(arg)}} ? {{caller()}} : {{alt}}{%- else -%}{{caller()}}{%- endif -%}
{% endmacro %}

{% macro opt_if(arg) %}
    {% if arg is opt %}
    if ({{opt_cond(arg)}}) {
    {{caller()|indent-}}
    {{"}"|indent(first = true)}}
    {% else %}
{{caller()-}}
    {% endif %}
{% endmacro %}

{% macro opt_cuda_cond(arg) %}
    {%- if arg is opt -%}optflags & 1 << {{lh.optIdx(arg)}}{%- endif -%}
{% endmacro %}

{% macro opt_cuda_cond_comp(arg) %}
    {%- if arg is opt -%}{{opt_cuda_cond(arg)}} && {% endif -%}
{% endmacro %}

{% macro opt_cuda_tern(arg, alt = "NULL") %}
    {%- if arg is opt -%}{{opt_cuda_cond(arg)}} ? {{caller()}} : {{alt}}{%- else -%}{{caller()}}{%- endif -%}
{% endmacro %}

{% macro opt_cuda_if(arg) %}
    {% if arg is opt %}
    if ({{opt_cuda_cond(arg)}}) {
    {{caller()|indent-}}
    {{"}"|indent(first = true)}}
    {% else %}
{{caller()-}}
    {% endif %}
{% endmacro %}

{% macro map_lookup(arg, kernel_idx = '') -%}
{%- if arg is double_indirect -%}
map{{arg.map_id}}[opp_k{{kernel_idx}}_map{{lh.map(arg).id}}_stride_d * {{arg.map_idx}} + opp_p2c[0]]
    {{-(" * %d" % lh.dat(arg).dim) if lh.dat(arg) is not soa}}
{%- else -%}
map{{arg.map_id}}[opp_k{{kernel_idx}}_map{{lh.map(arg).id}}_stride_d * {{arg.map_idx}} + n]
    {{-(" * %d" % lh.dat(arg).dim) if lh.dat(arg) is not soa}}
{%- endif -%}
{%- endmacro %}

{% macro arg_to_pointer_cuda(arg) -%}

    {%- if arg is gbl %}
        {% if arg is reduction %}
gbl{{arg.id}}{{"_local"}}
        {%- else -%}
        {%- set cast = arg.typ -%}
gbl{{arg.id}}
        {%- endif -%}
    {%- else -%}
        {%- set cast = lh.dat(arg).typ -%}
        
        {%- if arg is direct -%}
            {%- if lh is injected_loop and arg.offset -%}
            {%- set offset = " + (inj_start + n)" -%}
            {%- else -%}
            {%- set offset = " + n" -%}
            {%- endif -%}
            dat{{lh.dat(arg).id}}{{offset}}
        {%- else -%}
            {%- if arg is reduction -%}
            arg{{arg.id}}_{{arg.map_idx}}_local
            {%- else -%}
                {%- if arg is double_indirect -%}
            {%- set offset = " + map%s[opp_p2c[0] + opp_k%d_map%d_stride_d * %d]" % (arg.map_id, kernel_idx, arg.map_id, arg.map_idx) -%}
                {%- elif arg is p2c_mapped -%}
            {%- set offset = " + opp_p2c[0]" -%}
                {%- elif arg is indirect -%}
            {%- set offset = " + map%s[n + opp_k%d_map%d_stride_d * %d]" % (arg.map_id, kernel_idx, lh.map(arg).id, arg.map_idx) -%}
                {%- endif -%}
                dat{{lh.dat(arg).id}}{{offset}}
            {%- endif -%}
        {%- endif -%}

            
    {%- endif -%}
{%- endmacro %}

{% block prologue %}
{{super()}}
    {% for dat in lh.dats|soa %}
OPP_INT opp_k{{kernel_idx}}_dat{{dat.id}}_stride = -1;
    {% endfor %}
    {% for map in lh.maps %}
OPP_INT opp_k{{kernel_idx}}_map{{map.id}}_stride = -1;
    {% endfor %}

    {% for dat in lh.dats|soa %}
__constant__ OPP_INT opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d;
    {% endfor %}
    {% for map in lh.maps %}
__constant__ OPP_INT opp_k{{kernel_idx}}_map{{map.id}}_stride_d;
    {% endfor %}

    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
OPP_INT opp_k{{kernel_idx}}_sr_set_stride = -1;
__constant__ OPP_INT opp_k{{kernel_idx}}_sr_set_stride_d;
    {% endif%}

    {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) if config.seg_red %}
thrust::device_vector<OPP_INT> sr_dat{{lh.dat(arg).id}}_keys_dv;
thrust::device_vector<{{lh.dat(arg).typ}}> sr_dat{{lh.dat(arg).id}}_values_dv;
    {% endfor %}
{% endblock %}

{% macro atomic_kernel_wrapper() -%}
__global__ void opp_dev_{{lh.kernel}}(
    {{-"\n    const unsigned optflags," if lh.args|opt|length > 0}}
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) and lh is particle_loop %}
    {{dat.typ}} **__restrict__ dat{{dat.id}}, const OPP_INT dat{{dat.id}}_arr_count, // {{dat.ptr}}
        {% else %}
    {{"const " if dat is read_in(lh)}}{{dat.typ}} *__restrict__ dat{{dat.id}},  // {{dat.ptr}}
        {% endif %}
    {% endfor %}
    {% if lh is p2c_mapped %}
    const OPP_INT *__restrict__ p2c_map,
    {% endif %}
    {% for map in lh.maps %}
    const OPP_INT *__restrict__ map{{map.id}},  // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    {{"const " if arg.access_type == OP.AccessType.Read}}{{arg.typ}} *gbl{{arg.id}},
    {% endfor %}
    {% if lh is injected_loop %}
    const OPP_INT inj_start,
    {% endif %}
    const OPP_INT start,
    const OPP_INT end
) 
{
    {% for arg in lh.args|gbl|reduction %}
    {{arg.typ}} gbl{{arg.id}}_local[{{arg.dim}}];
    for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{arg.dim}}; ++d)
        gbl{{arg.id}}_local[d] = {% if arg is inc -%}
            {{arg.typ}}_ZERO
        {%- else -%}
            gbl{{arg.id}}[blockIdx.x * {{arg.dim}} + d]
        {%- endif -%};

    {% endfor %}
    const int thread_id = threadIdx.x + blockIdx.x * blockDim.x;

    {% if lh is direct %}
    for (int n = thread_id; n < (end - start); n += blockDim.x * gridDim.x) {

        opp_k{{kernel_idx}}::{{lh.kernel}}(
            {% for arg in lh.args %}
            {%+ call opt_cuda_tern(arg) %}{{arg_to_pointer_cuda(arg)}}{% endcall %}{{-"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif +%}
            {% endfor %}
        );
    }
    {% else %}
        {% if lh.args|gbl|reduction|length > 0 %}
    for (int n = thread_id; n < (end - start); n += blockDim.x * gridDim.x) {
        {% else %}
    if (thread_id + start < end) {

        const int n = {{"thread_id + start" if config.atomics else "col_reord[thread_id + start]"}};
        {% endif %}
        {% if lh is p2c_mapped %}
        
            {% if lh is injected_loop %}
        const OPP_INT* opp_p2c = p2c_map + (inj_start + n);
            {% else %}
        const OPP_INT* opp_p2c = p2c_map + n;
            {% endif %}
        {% endif %}
        
        {% for arg in lh.args_expanded|indirect|reduction if config.atomics %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_{{arg.map_idx}}_local[{{lh.dat(arg).dim}}];
        for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            arg{{arg.id}}_{{arg.map_idx}}_local[d] = {{lh.dat(arg).typ}}_ZERO;

        {% endfor %}
        opp_k{{kernel_idx}}::{{lh.kernel}}(
            {% for arg in lh.args %}
            {%+ call opt_cuda_tern(arg) %}{{arg_to_pointer_cuda(arg)}}{% endcall %}{{-"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif +%}
            {% endfor %}
        );
        {% for dat in lh.dats if config.atomics and lh is particle_loop %}
            {% if dat is indirect_reduction(lh) %}

        {{dat.typ}}* tmp{{dat.id}} = dat{{dat.id}}[threadIdx.x % dat{{dat.id}}_arr_count];
            {% endif %}
        {% endfor %}
        {%- if lh is particle_loop and config.atomics -%}
        {% set dat_name = "tmp" -%}
        {%- else %}
        {% set dat_name = "dat" -%}
        {%- endif %}
        {% for arg in lh.args_expanded|indirect|reduction if config.atomics %}

        for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            atomicAdd({{dat_name}}{{arg.dat_id}} + {{map_lookup(arg, kernel_idx)}} + (d{{stride_cuda(arg)}}), arg{{arg.id}}_{{arg.map_idx}}_local[d]);
        {% endfor %}
    }
    {% endif %}
    {% for arg in lh.args|gbl|reduction %}

    for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{arg.dim}}; ++d)
        opp_reduction<OPP_{{arg.access_type.name}}>(gbl{{arg.id}} + blockIdx.x * {{arg.dim}} + d, gbl{{arg.id}}_local[d]);
    {% endfor %}
    {{ caller() }}
}
{% endmacro %}

{% macro seg_red_kernel_wrapper() -%}
__global__ void opp_dev_sr_{{lh.kernel}}( // Used for Segmented Reductions
    {{-"\n    const unsigned optflags," if lh.args|opt|length > 0}}
    {% for dat in lh.dats %}
        {% if dat is sr_both_direct_and_indirect_reduction(lh) %}
    OPP_INT *__restrict__ sr_dat{{dat.id}}_keys,     // sr keys for {{dat.ptr}}
    {{dat.typ}} *__restrict__ sr_dat{{dat.id}}_values,     // sr values for {{dat.ptr}}
    {{"const " if dat is read_in(lh)}}{{dat.typ}} *__restrict__ dat{{dat.id}},  // {{dat.ptr}}
        {% else %}
            {% if dat is indirect_reduction(lh) %}
    OPP_INT *__restrict__ sr_dat{{dat.id}}_keys,     // sr keys for {{dat.ptr}}
    {{dat.typ}} *__restrict__ sr_dat{{dat.id}}_values,     // sr values for {{dat.ptr}}
            {% else %}
    {{"const " if dat is read_in(lh)}}{{dat.typ}} *__restrict__ dat{{dat.id}},  // {{dat.ptr}}
            {% endif %}
        {% endif %}
    {% endfor %}
    {% if lh is p2c_mapped %}
    const OPP_INT *__restrict__ p2c_map,
    {% endif %}
    {% for map in lh.maps %}
    const OPP_INT *__restrict__ map{{map.id}},  // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    {{"const " if arg.access_type == OP.AccessType.Read}}{{arg.typ}} *gbl{{arg.id}},
    {% endfor %}
    {% if lh is injected_loop %}
    const OPP_INT inj_start,
    {% endif %}
    const OPP_INT start,
    const OPP_INT end
) 
{
    {% for arg in lh.args|gbl|reduction %}
    {{arg.typ}} gbl{{arg.id}}_local[{{arg.dim}}];
    for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{arg.dim}}; ++d)
        gbl{{arg.id}}_local[d] = {% if arg is inc -%}
            {{arg.typ}}_ZERO
        {%- else -%}
            gbl{{arg.id}}[blockIdx.x * {{arg.dim}} + d]
        {%- endif -%};

    {% endfor %}
    const int thread_id = threadIdx.x + blockIdx.x * blockDim.x;

    if (thread_id + start < end) {

        const int n = thread_id + start;
        {% if lh is p2c_mapped %}
        
            {% if lh is injected_loop %}
        const OPP_INT* opp_p2c = p2c_map + (inj_start + n);
            {% elif lh is p2c_mapped %}
        const OPP_INT* opp_p2c = p2c_map + n;
            {% endif %}
        {% endif %}

        {% for arg in lh.args_expanded|indirect|reduction if config.atomics %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_{{arg.map_idx}}_local[{{lh.dat(arg).dim}}];
        for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            arg{{arg.id}}_{{arg.map_idx}}_local[d] = {{lh.dat(arg).typ}}_ZERO;

        {% endfor %}
        opp_k{{kernel_idx}}::{{lh.kernel}}(
            {% for arg in lh.args %}
            {%+ call opt_cuda_tern(arg) %}{{arg_to_pointer_cuda(arg)}}{% endcall %}{{-"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif +%}
            {% endfor %}
        );

        int offset = 0;
        {% for arg in lh.args_expanded|indirect|reduction if config.atomics %}
        for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d, ++offset) {
            {#% for datx in lh.dats|same_iter_set_dat(lh) %#}
            sr_dat{{arg.dat_id}}_values[n + opp_k{{kernel_idx}}_sr_set_stride_d * offset] = arg{{arg.id}}_{{arg.map_idx}}_local[d];
            sr_dat{{arg.dat_id}}_keys[n + opp_k{{kernel_idx}}_sr_set_stride_d * offset] = {{map_lookup(arg, kernel_idx)}} + (d{{stride_cuda(arg)}});
            {#% endfor %#}
        }
        {% endfor %}
    }
    {% for arg in lh.args|gbl|reduction %}

    for (int d = 0; {{opt_cuda_cond_comp(arg)}}d < {{arg.dim}}; ++d) 
        opp_reduction<OPP_{{arg.access_type.name}}>(gbl{{arg.id}} + blockIdx.x * {{arg.dim}} + d, gbl{{arg.id}}_local[d]);
    {% endfor %}
    {{ caller() }}
}

{% endmacro %}

{% block kernel_wrapper %}
    {% if lh is direct or config.atomics %}
//--------------------------------------------------------------
        {% call atomic_kernel_wrapper() %}
        {% endcall %}
    {% endif %}

//--------------------------------------------------------------
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
        {% call seg_red_kernel_wrapper() %}
        {% endcall %}
    {% endif %}
{% endblock %}

{% block host_prologue_early_exit_cleanup %}
        opp_set_dirtybit_grouped(nargs, args, Device_GPU);
        OPP_DEVICE_SYNCHRONIZE();   
{% endblock %}

{% block host_prologue %}
void opp_par_loop_{{lh.iterator_type}}__{{lh.kernel}}(opp_set set,
    {% for arg in lh.args %}
    opp_arg arg{{arg.id}}{{"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif -%} | OPP_{{arg.access_type.name}}
    {% endfor %}
) 
{ OPP_RETURN_IF_INVALID_PROCESS;

    const int nargs = {{lh.args|length}};
    opp_arg args[nargs];

    {% for arg in lh.args %}
    args[{{loop.index0}}] = {{arg_dat_redef(arg) if lh.args[arg.id] is vec else "arg%d" % arg.id}};
    {% endfor %}

    opp_profiler->start("{{lh.kernel}}");

    if (OPP_DBG) opp_printf("APP", "opp_par_loop_{{lh.iterator_type}}__{{lh.kernel}} set_size %d", set->size);

    {% if lh is injected_loop %}
    opp_mpi_halo_exchanges_grouped(set, nargs, args, Device_GPU);
    const int iter_size = set->diff; 
    const int inj_start = (set->size - set->diff);  
    {% else %}
    const int iter_size = opp_mpi_halo_exchanges_grouped(set, nargs, args, Device_GPU);
        {% if lh is double_indirect_reduc %}

#ifdef USE_MPI
    opp_init_double_indirect_reductions_device(nargs, args);
#endif
        {% endif %} 
    {% endif %} 
    {% if lh.args|opt|length > 0 %}
    unsigned optflags = 0;
    {% for arg in lh.args|opt %}
        {% call opt_if(arg) %}
    optflags |= 1 << {{lh.optIdx(arg)}};
        {% endcall %}
    {% endfor %}
    {% endif %}
    {% for arg in lh.args|gbl %}
    {{arg.typ}} *arg{{arg.id}}_host_data = ({{arg.typ}} *)args[{{arg.id}}].data;{{"\n" if loop.last}}
    {% endfor %}
    {% if lh.args|gbl|read_or_write|length > 0 %}
    int const_bytes = 0;

        {% for arg in lh.args|gbl|read_or_write %}
            {% call opt_if(arg) %}
    const_bytes += ROUND_UP({{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_reallocConstArrays(const_bytes);
    const_bytes = 0;

        {% for arg in lh.args|gbl|read_or_write %}
            {% call opt_if(arg) %}
    args[{{arg.id}}].data   = OPP_consts_h + const_bytes;
    args[{{arg.id}}].data_d = OPP_consts_d + const_bytes;

    for (int d = 0; d < {{arg.dim}}; ++d)
        (({{arg.typ}} *)args[{{arg.id}}].data)[d] = arg{{arg.id}}_host_data[d];

    const_bytes += ROUND_UP({{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}

        {% endfor %}
    opp_mvConstArraysToDevice(const_bytes);

    {% endif %}
    {% for dat in lh.dats|soa %}
        {% if dat is map_not_dat %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d, &opp_k{{kernel_idx}}_dat{{dat.id}}_stride, &(args[{{dat.arg_id}}].size), 1);
        {% else %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d, &opp_k{{kernel_idx}}_dat{{dat.id}}_stride, &(args[{{dat.arg_id}}].dat->set->set_capacity), 1);
        {% endif %}
    {% endfor %}
    {% for map in lh.maps %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_map{{map.id}}_stride_d, &opp_k{{kernel_idx}}_map{{map.id}}_stride, &(args[{{map.arg_id}}].size), 1);
    {% endfor %}

#ifdef OPP_BLOCK_SIZE_{{kernel_idx}}
    const int block_size = OPP_BLOCK_SIZE_{{kernel_idx}};
#else
    const int block_size = OPP_gpu_threads_per_block;
#endif

    opp_mpi_halo_wait_all(nargs, args);

    int num_blocks = 200;
    {% if lh.args|gbl|reduction|length > 0 %}

        {% if lh is direct %}
    int max_blocks = num_blocks;
        {% elif config.atomics %}
    int max_blocks = (MAX(set->core_size, set->size + set->exec_size - set->core_size) - 1) / block_size + 1;
        {% else %}
    // Get SR implementation here
        {% endif %}

    int reduction_bytes = 0;
    int reduction_size = 0;

        {% for arg in lh.args|gbl|reduction %}
            {% call opt_if(arg) %}
    reduction_bytes += ROUND_UP(max_blocks * {{arg.dim}} * sizeof({{arg.typ}}));
    reduction_size   = MAX(reduction_size, sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_reallocReductArrays(reduction_bytes);
    reduction_bytes = 0;

        {% for arg in lh.args|gbl|reduction %}
            {% call opt_if(arg) %}
    args[{{arg.id}}].data   = OPP_reduct_h + reduction_bytes;
    args[{{arg.id}}].data_d = OPP_reduct_d + reduction_bytes;

    for (int b = 0; b < max_blocks; ++b) {
        for (int d = 0; d < {{arg.dim}}; ++d)
            (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d] = {% if arg.access_type == OP.AccessType.INC -%}
                {{arg.typ}}_ZERO
            {%- else -%}
                arg{{arg.id}}_host_data[d]
            {%- endif %};
    }

    reduction_bytes += ROUND_UP(max_blocks * {{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_mvReductArraysToDevice(reduction_bytes);
    
    {% endif %}
{% endblock %}

{% macro atomic_kernel_call(shared_size) %}
opp_dev_{{lh.kernel}}<<<num_blocks, block_size{{-(", %s" % shared_size) if lh.args|gbl|reduction|length > 0}}>>>(
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) and lh is particle_loop %}
    arg{{dat.arg_id}}_dat_thread_data_d, array_count,     // {{dat.ptr}}
        {% else %}
    ({{dat.typ}} *)args[{{dat.arg_id}}].data_d,     // {{dat.ptr}}
        {% endif %}
    {% endfor %}
    {% if lh is p2c_mapped %}
    (OPP_INT *)set->mesh_relation_dat->data_d,
    {% endif %}
    {% for map in lh.maps %}
    args[{{map.arg_id}}].map_data_d,     // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    ({{arg.typ}} *)args[{{arg.id}}].data_d,
    {% endfor %}
    {% if lh is injected_loop %}
    inj_start,
    {% endif %}
    {% for extra_arg in varargs %}
    {{extra_arg}}{{"," if not loop.last}}
    {% endfor %}
);
{%- endmacro %}

{% macro seg_red_kernel_call(shared_size) %}
opp_dev_sr_{{lh.kernel}}<<<num_blocks, block_size{{-(", %s" % shared_size) if lh.args|gbl|reduction|length > 0}}>>>( 
    {% for dat in lh.dats %}
        {% if dat is sr_both_direct_and_indirect_reduction(lh) %}
    opp_get_dev_raw_ptr<OPP_INT>(sr_dat{{dat.id}}_keys_dv),     // sr keys for {{dat.ptr}}
    opp_get_dev_raw_ptr<{{dat.typ}}>(sr_dat{{dat.id}}_values_dv),     // sr values for {{dat.ptr}}
    ({{dat.typ}} *)args[{{dat.arg_id}}].data_d,     // {{dat.ptr}}
        {% else %}
            {% if dat is indirect_reduction(lh) %}
    opp_get_dev_raw_ptr<OPP_INT>(sr_dat{{dat.id}}_keys_dv),     // sr keys for {{dat.ptr}}
    opp_get_dev_raw_ptr<{{dat.typ}}>(sr_dat{{dat.id}}_values_dv),     // sr values for {{dat.ptr}}
            {% else %}
    ({{dat.typ}} *)args[{{dat.arg_id}}].data_d,     // {{dat.ptr}}
            {% endif %}
        {% endif %}
    {% endfor %}
    {% if lh is p2c_mapped %}
    (OPP_INT *)set->mesh_relation_dat->data_d,
    {% endif %}
    {% for map in lh.maps %}
    args[{{map.arg_id}}].map_data_d,     // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    ({{arg.typ}} *)args[{{arg.id}}].data_d,
    {% endfor %}
    {% if lh is injected_loop %}
    inj_start,
    {% endif %}
    {% for extra_arg in varargs %}
    {{extra_arg}}{{"," if not loop.last}}
    {% endfor %}
);
{%- endmacro %}

{% block host_loop %}
    if (iter_size > 0) 
    {
        const OPP_INT start = 0;
        const OPP_INT end = iter_size;
        {% if lh.args|gbl|reduction|length <= 0 %}
        num_blocks = (end - start - 1) / block_size + 1;

        {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.atomics and config.seg_red %}
        if (!opp_params->get<OPP_BOOL>("use_reg_red")) // Do atomics ----------       
    {% endif %}
    {% if lh is direct or config.atomics %}
        {
        {% if lh.args|dat|indirect|reduction|length > 0 and lh is particle_loop %}
            const int array_count = opp_params->get<OPP_INT>("gpu_reduction_arrays");
        {% endif %}
        {% for dat in lh.dats %}
            {% if dat is indirect_reduction(lh) and lh is particle_loop %}
            {{dat.typ}}** arg{{dat.id}}_dat_thread_data_d = opp_create_thread_level_data<{{dat.typ}}>(args[{{dat.arg_id}}]);
            {% endif %}
        {% endfor %}
            {{atomic_kernel_call("(reduction_size * block_size)",
                "start", "end")|indent(12)}}
        {% if lh.args|dat|indirect|reduction|length > 0 and lh is particle_loop %}
            OPP_DEVICE_SYNCHRONIZE(); 
        {% endif %}
        {% for dat in lh.dats %}
            {% if dat is indirect_reduction(lh) and lh is particle_loop %}
            opp_reduce_thread_level_data<{{dat.typ}}>(args[{{dat.id}}]);
            {% endif %}
        {% endfor %}
        }
    {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.atomics and config.seg_red %}     
        else // Do segmented reductions ----------       
    {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
        {
            opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_sr_set_stride_d, &opp_k{{kernel_idx}}_sr_set_stride, &(set->size), 1);

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
            size_t operating_size_dat{{arg.dat_id}} = 0, resize_size_dat{{arg.dat_id}} = 0;
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction %}
            operating_size_dat{{arg.dat_id}} += (size_t)(args[{{arg.id}}].dat->dim);
            resize_size_dat{{arg.dat_id}} += (size_t)(args[{{arg.id}}].dat->dim);
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
            operating_size_dat{{arg.dat_id}} *= (size_t)(set->size);
            resize_size_dat{{arg.dat_id}} *= (size_t)(set->set_capacity);
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
            if (resize_size_dat{{arg.dat_id}} > sr_dat{{arg.dat_id}}_keys_dv.size()) { // resize only if current vector is small        
                sr_dat{{arg.dat_id}}_keys_dv.resize(resize_size_dat{{arg.dat_id}}, 0);
                sr_dat{{arg.dat_id}}_values_dv.resize(resize_size_dat{{arg.dat_id}}, 0);
            }
        {% endfor %}
        
            // Create key/value pairs
            opp_profiler->start("SR_CrKeyVal");
            {{seg_red_kernel_call("(reduction_size * block_size)",
                "start", "end")|indent(12)}}
            OPP_DEVICE_SYNCHRONIZE();
            opp_profiler->end("SR_CrKeyVal");

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
            // Sort by keys to bring the identical keys together
            opp_profiler->start("SR_SortByKey");
            thrust::sort_by_key(sr_dat{{arg.dat_id}}_keys_dv.begin(), sr_dat{{arg.dat_id}}_keys_dv.begin() + operating_size_dat{{arg.dat_id}}, 
                sr_dat{{arg.dat_id}}_values_dv.begin());
            opp_profiler->end("SR_SortByKey");

            // Compute the unique keys and their corresponding values
            opp_profiler->start("SR_RedByKey");
            auto new_end = thrust::reduce_by_key(
                sr_dat{{arg.dat_id}}_keys_dv.begin(), sr_dat{{arg.dat_id}}_keys_dv.begin() + operating_size_dat{{arg.dat_id}},
                sr_dat{{arg.dat_id}}_values_dv.begin(),
                sr_dat{{arg.dat_id}}_keys_dv.begin(),
                sr_dat{{arg.dat_id}}_values_dv.begin());        
            opp_profiler->end("SR_RedByKey");

            const size_t reduced_size = (new_end.first - sr_dat{{arg.dat_id}}_keys_dv.begin());
            
            // Assign reduced values to the nodes using keys/values
            opp_profiler->start("SR_Assign");                
            opp_k{{kernel_idx}}::assign_values<<<num_blocks, block_size>>> ( // TODO : check whether num_blocks is correct
                opp_get_dev_raw_ptr<OPP_INT>(sr_dat{{arg.dat_id}}_keys_dv),
                opp_get_dev_raw_ptr<OPP_REAL>(sr_dat{{arg.dat_id}}_values_dv),
                (OPP_REAL *) args[{{arg.dat_id}}].data_d,
                0, reduced_size);
            OPP_DEVICE_SYNCHRONIZE();
            opp_profiler->end("SR_Assign");

            // Last: clear the thrust vectors if this is the last iteration (avoid crash)
            if (opp_params->get<OPP_INT>("num_steps") == (OPP_main_loop_iter + 1)) {
                OPP_DEVICE_SYNCHRONIZE();
                sr_dat{{arg.dat_id}}_values_dv.clear(); sr_dat{{arg.dat_id}}_values_dv.shrink_to_fit();
                sr_dat{{arg.dat_id}}_keys_dv.clear(); sr_dat{{arg.dat_id}}_keys_dv.shrink_to_fit();
            }        
        {% endfor %}
        }
    {% endif %}
    }
{% endblock %}

{% block host_epilogue %}
    {% if lh.args|gbl|read_write|length > 0 or lh.args|gbl|write|length > 0 %}
    mvConstArraysToHost(const_bytes);
    
        {% for arg in lh.args|gbl if arg is write or arg is read_write %}
    for (int d = 0; d < {{arg.dim}}; ++d)
        arg{{arg.id}}_host_data[d]; = (({{arg.typ}} *)args[{{arg.id}}].data)[d];
        {% endfor %}
    {% endif %}
    {% for arg in lh.args|gbl|read_or_write %}
    args[{{arg.id}}].data = (char *)arg{{arg.id}}_host_data;{{"\n" if loop.last}}
    {% endfor %}
    {% if lh.args|gbl|reduction|length > 0 %}

    opp_mvReductArraysToHost(reduction_bytes);

    {% endif %}
    {% for arg in lh.args|gbl|reduction %}
    for (int b = 0; {{opt_cond_comp(arg)}}b < max_blocks; ++b) {
        for (int d = 0; d < {{arg.dim}}; ++d)
        {% if arg.access_type == OP.AccessType.INC %}
            arg{{arg.id}}_host_data[d] += (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d];
        {% elif arg.access_type in [OP.AccessType.MIN, OP.AccessType.MAX] %}
            arg{{arg.id}}_host_data[d] = {{arg.access_type.name-}}
                (arg{{arg.id}}_host_data[d], (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d]);
        {% endif %}
    }
    {% endfor %}

    {% for arg in lh.args|gbl|reduction %}
        {% call opt_if(arg) %}
    args[{{arg.id}}].data = (char *)arg{{arg.id}}_host_data;
    opp_mpi_reduce(&args[{{arg.id}}], arg{{arg.id}}_host_data);
        {% endcall %}

    {% endfor %}
    opp_set_dirtybit_grouped(nargs, args, Device_GPU);
    OPP_DEVICE_SYNCHRONIZE();   
    {% if lh is double_indirect_reduc %}

#ifdef USE_MPI    
    opp_exchange_double_indirect_reductions_device(nargs, args);
    opp_complete_double_indirect_reductions_device(nargs, args);
#endif
    {% endif %} 
{{super()}}
{% endblock %}