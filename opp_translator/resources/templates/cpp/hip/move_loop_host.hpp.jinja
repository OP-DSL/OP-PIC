{% extends "cpp/loop_host.hpp.jinja" %}

{% block kernel %}

namespace opp_k{{kernel_idx}} {
{% if lh.dh_loop_required %}

namespace host {
{{host_kernel_func}}
}

{% endif %}
{{kernel_func}}
}

{% endblock %}

{% macro stride_device(arg) -%}
{{-" * opp_k%s_dat%s_stride_d" % (kernel_idx, arg.dat_id) if lh.dat(arg) is soa-}}
{%- endmacro %}

{% macro opt_cond(arg) %}
    {%- if arg is opt -%}arg{{arg.id}}.opt{%- endif -%}
{% endmacro %}

{% macro opt_cond_comp(arg) %}
    {%- if arg is opt -%}{{opt_cond(arg)}} && {% endif -%}
{% endmacro %}

{% macro opt_tern(arg, alt = "NULL") %}
    {%- if arg is opt -%}{{opt_cond(arg)}} ? {{caller()}} : {{alt}}{%- else -%}{{caller()}}{%- endif -%}
{% endmacro %}

{% macro opt_if(arg) %}
    {% if arg is opt %}
    if ({{opt_cond(arg)}}) {
    {{caller()|indent-}}
    {{"}"|indent(first = true)}}
    {% else %}
{{caller()-}}
    {% endif %}
{% endmacro %}

{% macro opt_device_cond(arg) %}
    {%- if arg is opt -%}optflags & 1 << {{lh.optIdx(arg)}}{%- endif -%}
{% endmacro %}

{% macro opt_device_cond_comp(arg) %}
    {%- if arg is opt -%}{{opt_device_cond(arg)}} && {% endif -%}
{% endmacro %}

{% macro opt_device_tern(arg, alt = "NULL") %}
    {%- if arg is opt -%}{{opt_device_cond(arg)}} ? {{caller()}} : {{alt}}{%- else -%}{{caller()}}{%- endif -%}
{% endmacro %}

{% macro opt_device_if(arg) %}
    {% if arg is opt %}
    if ({{opt_device_cond(arg)}}) {
    {{caller()|indent-}}
    {{"}"|indent(first = true)}}
    {% else %}
{{caller()-}}
    {% endif %}
{% endmacro %}

{% macro map_lookup(arg, kernel_idx = '') -%}
{%- if arg is double_indirect -%}
map{{arg.map_id}}[opp_k{{kernel_idx}}_map{{lh.map(arg).id}}_stride_d * {{arg.map_idx}} + p2c]
    {{-(" * %d" % lh.dat(arg).dim) if lh.dat(arg) is not soa}}
{%- elif arg is p2c_mapped -%}
p2c
    {{-(" * %d" % lh.dat(arg).dim) if lh.dat(arg) is not soa}}
{%- else -%}
map{{arg.map_id}}[opp_k{{kernel_idx}}_map{{lh.map(arg).id}}_stride_d * {{arg.map_idx}} + n]
    {{-(" * %d" % lh.dat(arg).dim) if lh.dat(arg) is not soa}}
{%- endif -%}    
{%- endmacro %}

{% macro arg_to_pointer_device(arg) -%}

    {%- if arg is gbl %}
        {% if arg is reduction %}
gbl{{arg.id}}{{"_local"}}
        {%- else -%}
        {%- set cast = arg.typ -%}
gbl{{arg.id}}
        {%- endif -%}
    {%- else -%}
        {%- set cast = lh.dat(arg).typ -%}
        
        {%- if arg is direct -%}
            {%- if lh is injected_loop -%}
            {%- set offset = " + (inj_start + n)" -%}
            {%- else -%}
            {%- set offset = " + n" -%}
            {%- endif -%}
            dat{{lh.dat(arg).id}}{{offset}}
        {%- else -%}
            {%- if arg is reduction -%}
                {%- if arg is p2c_mapped and not double_indirect -%}
            arg{{arg.id}}_p2c_local
                {%- else -%}
            arg{{arg.id}}_{{arg.map_idx}}_local
                {%- endif -%}
            {%- else -%}
                {%- if arg is double_indirect -%}
            {%- set offset = " + map%s[p2c + opp_k%d_map%d_stride_d * %d]" % (arg.map_id, kernel_idx, arg.map_id, arg.map_idx) -%}
                {%- elif arg is p2c_mapped -%}
            {%- set offset = " + p2c" -%}
                {%- elif arg is indirect -%}
            {%- set offset = " + map%s[n + opp_k%d_map%d_stride_d * %d]" % (arg.map_id, kernel_idx, lh.map(arg).id, arg.map_idx) -%}
                {%- endif -%}
                dat{{lh.dat(arg).id}}{{offset}}
            {%- endif -%}
        {%- endif -%}         
    {%- endif -%}
{%- endmacro %}

{%- macro arg_to_pointer_dh(arg, lh) %}
    {%- if arg.id == 0 -%}
            (const OPP_REAL*)&point,
    {%- elif arg is gbl %}
        {%- set cast = arg.typ -%}
        ({{cast}} *)args[{{arg.id}}].data{%-if arg.id+1 != lh.args|length-%},{%-endif-%}
    {%- else -%}

        {%- if arg is direct -%}
            {%- set offset = "" -%}
        {%- elif arg is double_indirect -%}
            {%- set offset = " + (map%s[%d] * %d)" % (arg.map_id, arg.map_idx, lh.dat(arg).dim) -%}
        {%- elif arg is indirect -%}
            {%- set offset = " + (temp_ci * %d)" % (lh.dat(arg).dim) -%}
        {%- endif -%}

        {# 0 is OP.AccessType.READ #}
        {%- if arg.access_type.value == 0 -%} 
            (const {{lh.dat(arg).typ}} *)args[{{arg.id}}].data{{offset}}{%-if arg.id+1 != lh.args|length-%},{%-endif%} // {% if arg is dat -%}{{lh.dat(arg).ptr}} {%- endif -%} | OPP_{{arg.access_type.name}}
        {%- else -%}
            arg{{arg.id}}_temp{%-if arg.id+1 != lh.args|length-%},{%-endif%} // {% if arg is dat -%}{{lh.dat(arg).ptr}} {%- endif -%} | OPP_{{arg.access_type.name}}
        {%- endif -%}
    {%- endif -%}
{%- endmacro -%}

{% block prologue %}
{{super()}}
    {% for dat in lh.dats|soa %}
OPP_INT opp_k{{kernel_idx}}_dat{{dat.id}}_stride = -1;
    {% endfor %}
    {% for map in lh.maps %}
OPP_INT opp_k{{kernel_idx}}_map{{map.id}}_stride = -1;
    {% endfor %}
OPP_INT opp_k{{kernel_idx}}_c2c_map_stride = -1;

    {% for dat in lh.dats|soa %}
__constant__ OPP_INT opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d;
    {% endfor %}
    {% for map in lh.maps %}
__constant__ OPP_INT opp_k{{kernel_idx}}_map{{map.id}}_stride_d;
    {% endfor %}
__constant__ OPP_INT opp_k{{kernel_idx}}_c2c_map_stride_d;

    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
// Segmented Reductions Structures 
// --------------------------------------------------------------
OPP_INT opp_k{{kernel_idx}}_sr_set_stride = -1;
__constant__ OPP_INT opp_k{{kernel_idx}}_sr_set_stride_d;
    {% endif%}

    {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) if config.seg_red %}
thrust::device_vector<OPP_INT> sr_dat{{lh.dat(arg).id}}_keys_dv;
thrust::device_vector<{{lh.dat(arg).typ}}> sr_dat{{lh.dat(arg).id}}_values_dv;

thrust::device_vector<OPP_INT> sr_dat{{lh.dat(arg).id}}_keys_dv2;
thrust::device_vector<{{lh.dat(arg).typ}}> sr_dat{{lh.dat(arg).id}}_values_dv2;
// --------------------------------------------------------------
    {% endfor %}
{% endblock %}

{% macro atomic_kernel_wrapper() -%}
__global__ void opp_dev_{{lh.kernel}}(
    {{-"\n    const unsigned optflags," if lh.args|opt|length > 0}}
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) and lh is particle_loop %}
    {{dat.typ}} **__restrict__ dat{{dat.id}}, const OPP_INT dat{{dat.id}}_arr_count,     // {{dat.ptr}}
        {% else %}
    {{"const " if dat is read_in(lh)}}{{dat.typ}} *__restrict__ dat{{dat.id}},     // {{dat.ptr}}
        {% endif %}
    {% endfor %}
    OPP_INT *__restrict__ p2c_map,
    const OPP_INT *__restrict__ c2c_map,
    {% for map in lh.maps %}
    const OPP_INT *__restrict__ map{{map.id}},     // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    {{"const " if arg.access_type == OP.AccessType.Read}}{{arg.typ}} *gbl{{arg.id}},
    {% endfor %}
    {% if lh is injected_loop %}
    const OPP_INT inj_start,
    {% endif %}
    OPP_INT *__restrict__ particle_remove_count,
    OPP_INT *__restrict__ particle_remove_indices,
    OPP_INT *__restrict__ move_particle_indices,
    OPP_INT *__restrict__ move_cell_indices,
    OPP_INT *__restrict__ move_count,
    const OPP_INT start,
    const OPP_INT end
) 
{
    {% for arg in lh.args|gbl|reduction %}
    {{arg.typ}} gbl{{arg.id}}_local[{{arg.dim}}];
    for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{arg.dim}}; ++d)
        gbl{{arg.id}}_local[d] = {% if arg is inc -%}
            {{arg.typ}}_ZERO
        {%- else -%}
            gbl{{arg.id}}[blockIdx.x * {{arg.dim}} + d]
        {%- endif -%};

    {% endfor %}
    const int n = OPP_DEVICE_GLOBAL_LINEAR_ID + start;

    if (n < end) {

        OPP_INT *opp_p2c = (p2c_map + n);
    {% if lh.dh_loop_required %}
        if (opp_p2c[0] == MAX_CELL_INDEX) {
            return;
        }
    {% endif %}

        char move_flag = OPP_NEED_MOVE;
        bool iter_one_flag = (OPP_comm_iteration_d > 0) ? false : true;

        {% for arg in lh.args_expanded|dat|indirect|reduction if config.atomics %}
            {% if arg is p2c_mapped and not double_indirect %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_p2c_local[{{lh.dat(arg).dim}}];
            {% else %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_{{arg.map_idx}}_local[{{lh.dat(arg).dim}}];
            {% endif %}

        {% endfor %}
        {% for dat in lh.dats if config.atomics and lh is particle_loop %}
            {% if dat is indirect_reduction(lh) %}
        {{dat.typ}}* tmp{{dat.id}} = dat{{dat.id}}[threadIdx.x % dat{{dat.id}}_arr_count];

            {% endif %}
        {% endfor %}
        do {
            const OPP_INT p2c = opp_p2c[0]; // get the value here, since the kernel might change it
            const OPP_INT* opp_c2c = c2c_map + p2c;           

        {% for arg in lh.args_expanded|dat|indirect|reduction if config.atomics %}
            for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            {% if arg is p2c_mapped and not double_indirect %}
                arg{{arg.id}}_p2c_local[d] = {{lh.dat(arg).typ}}_ZERO;
            {% else %}
                arg{{arg.id}}_{{arg.map_idx}}_local[d] = {{lh.dat(arg).typ}}_ZERO;
            {% endif %}

        {% endfor %}
            opp_k{{kernel_idx}}::{{lh.kernel}}(
                move_flag, iter_one_flag, opp_c2c, opp_p2c,
                {% for arg in lh.args %}
                {%+ call opt_device_tern(arg) %}{{arg_to_pointer_device(arg)}}{% endcall %}{{"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif +%}
                {% endfor %}          
            );

        {% if lh is particle_loop and config.atomics %}
        {% set dat_name = "tmp" -%}
        {% else %}
        {% set dat_name = "dat" -%}
        {% endif %}
        {% for arg in lh.args_expanded|dat|indirect|reduction if config.atomics %}
            for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            {% if arg is p2c_mapped and not double_indirect %}
                atomicAdd({{dat_name}}{{arg.dat_id}} + {{map_lookup(arg, kernel_idx)}} + (d{{stride_device(arg)}}), arg{{arg.id}}_p2c_local[d]);
            {% else %}
                atomicAdd({{dat_name}}{{arg.dat_id}} + {{map_lookup(arg, kernel_idx)}} + (d{{stride_device(arg)}}), arg{{arg.id}}_{{arg.map_idx}}_local[d]); // TODO: this looks incorrect
            {% endif %}        
        {% endfor %}
        } while (opp_part_check_status_device(move_flag, iter_one_flag, opp_p2c, n, 
            *particle_remove_count, particle_remove_indices, move_particle_indices, 
            move_cell_indices, move_count));        
    }
    {% for arg in lh.args|gbl|reduction %}

    for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{arg.dim}}; ++d)
        opp_reduction<OPP_{{arg.access_type.name}}>(gbl{{arg.id}} + blockIdx.x * {{arg.dim}} + d, gbl{{arg.id}}_local[d]);
    {% endfor %}
    {{ caller() }}
}
{% endmacro %}

{% macro seg_red_kernel_wrapper() -%}
__global__ void opp_dev_sr_{{lh.kernel}}( // Used for Segmented Reductions
    {% for dat in lh.dats %}
    {{"const " if dat is read_in(lh)}}{{dat.typ}} *__restrict__ dat{{dat.id}},     // {{dat.ptr}}
    {% endfor %}
    OPP_INT *__restrict__ p2c_map,
    const OPP_INT *__restrict__ c2c_map,
    {% for map in lh.maps %}
    const OPP_INT *__restrict__ map{{map.id}},     // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    {{"const " if arg.access_type == OP.AccessType.Read}}{{arg.typ}} *gbl{{arg.id}},
    {% endfor %}
    OPP_INT *__restrict__ particle_remove_count,
    OPP_INT *__restrict__ particle_remove_indices,
    OPP_INT *__restrict__ move_particle_indices,
    OPP_INT *__restrict__ move_cell_indices,
    OPP_INT *__restrict__ move_count,
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) %}
    {{dat.typ}} *__restrict__ sr_dat{{dat.id}}_values,     // sr values for {{dat.ptr}}
    OPP_INT *__restrict__ sr_dat{{dat.id}}_keys,     // sr keys for {{dat.ptr}}
        {% endif -%}  
    {% endfor %}
    const OPP_INT start,
    const OPP_INT end
) 
{
    const int n = OPP_DEVICE_GLOBAL_LINEAR_ID + start;

    if (n < end) {

        OPP_INT *opp_p2c = (p2c_map + n);
        char move_flag = OPP_NEED_MOVE;
        bool iter_one_flag = (OPP_comm_iteration_d > 0) ? false : true;

        {% for arg in lh.args_expanded|dat|indirect|reduction if config.seg_red %}
            {% if arg is p2c_mapped and not double_indirect %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_p2c_local[{{lh.dat(arg).dim}}];
            {% else %}
        {{lh.dat(arg).typ}} arg{{arg.id}}_{{arg.map_idx}}_local[{{lh.dat(arg).dim}}];
            {% endif %}

        {% endfor %}
        do {
            const OPP_INT p2c = opp_p2c[0]; // get the value here, since the kernel might change it
            const OPP_INT* opp_c2c = c2c_map + p2c;           

        {% for arg in lh.args_expanded|dat|indirect|reduction if config.seg_red %}
            for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            {% if arg is p2c_mapped and not double_indirect %}
                arg{{arg.id}}_p2c_local[d] = {{lh.dat(arg).typ}}_ZERO;
            {% else %}
                arg{{arg.id}}_{{arg.map_idx}}_local[d] = {{lh.dat(arg).typ}}_ZERO;
            {% endif %}

        {% endfor %}
            opp_k{{kernel_idx}}::{{lh.kernel}}(
                move_flag, iter_one_flag, opp_c2c, opp_p2c,
                {% for arg in lh.args %}
                {%+ call opt_device_tern(arg) %}{{arg_to_pointer_device(arg)}}{% endcall %}{{"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif +%}
                {% endfor %}          
            );

        {% for arg in lh.args_expanded|dat|indirect|reduction if config.seg_red %}
            if (iter_one_flag) {
                for (int d = 0; d < {{lh.dat(arg).dim}}; ++d) {
                    sr_dat{{arg.dat_id}}_values[n + opp_k{{kernel_idx}}_sr_set_stride_d * d] = arg{{arg.dat_id}}_p2c_local[d];              
                }
                sr_dat{{arg.dat_id}}_keys[n] = p2c; // TODO : Generate for double indirections too!
            }
            else {
                for (int d = 0; {{opt_device_cond_comp(arg)}}d < {{lh.dat(arg).dim}}; ++d)
            {% if arg is p2c_mapped and not double_indirect %}
                    atomicAdd(dat{{arg.dat_id}} + {{map_lookup(arg, kernel_idx)}} + (d{{stride_device(arg)}}), arg{{arg.id}}_p2c_local[d]);
            {% else %}
                    atomicAdd(dat{{arg.dat_id}} + {{map_lookup(arg, kernel_idx)}} + (d{{stride_device(arg)}}), arg{{arg.id}}_{{arg.map_idx}}_local[d]); // TODO: this looks incorrect
            {% endif %}
            }
        
        {% endfor %}
        } while (opp_part_check_status_device(move_flag, iter_one_flag, opp_p2c, n, 
            *particle_remove_count, particle_remove_indices, move_particle_indices, 
            move_cell_indices, move_count));        
    }
    {{ caller() }}
}
{% endmacro %}

{% block kernel_wrapper %}
    {% if lh is direct or config.atomics %}
//--------------------------------------------------------------
        {% call atomic_kernel_wrapper() %}
        {% endcall %}
    {% endif %}

    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
//--------------------------------------------------------------
        {% call seg_red_kernel_wrapper() %}
        {% endcall %}
    {% endif %}

{% endblock %}

{% block host_prologue_early_exit_cleanup %}
        opp_set_dirtybit_grouped(nargs, args, Device_GPU);
        OPP_DEVICE_SYNCHRONIZE();   
{% endblock %}

{% block host_prologue %}
void opp_particle_move__{{lh.kernel}}(opp_set set, opp_map c2c_map, opp_map p2c_map,
    {% for arg in lh.args %}
    opp_arg arg{{arg.id}}{{"," if not loop.last}}   // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif -%} | OPP_{{arg.access_type.name}}
    {% endfor %}
) 
{ OPP_RETURN_IF_INVALID_PROCESS;

    if (OPP_DBG) opp_printf("APP", "opp_particle_move__{{lh.kernel}} set_size %d", set->size);

    opp_profiler->start("{{lh.kernel}}");

    const int nargs = {{lh.args|length + 1}};
    opp_arg args[nargs];

    {% for arg in lh.args %}
    args[{{loop.index0}}] = {{arg_dat_redef(arg) if lh.args[arg.id] is vec else "arg%d" % arg.id}};
    {% endfor %}
    args[{{lh.args|length}}] = opp_arg_dat(p2c_map->p2c_dat, OPP_RW); // required to make dirty or should manually make it dirty

    opp_mpi_halo_exchanges_grouped(set, nargs, args, Device_GPU);
    {% if lh is double_indirect_reduc %}

#ifdef USE_MPI
    opp_init_double_indirect_reductions_device(nargs, args);
#endif
    {% endif %}

    const OPP_INT c2c_stride = c2c_map->from->size + c2c_map->from->exec_size + c2c_map->from->nonexec_size;

    opp_mem::dev_copy_to_symbol<OPP_INT>(OPP_cells_set_size_d, &OPP_cells_set_size, &(set->cells_set->size), 1);
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_c2c_map_stride_d, &opp_k{{kernel_idx}}_c2c_map_stride, &c2c_stride, 1);

    opp_mpi_halo_wait_all(nargs, args);
    {% for arg in lh.args|gbl %}
    {{arg.typ}} *arg{{arg.id}}_host_data = ({{arg.typ}} *)args[{{arg.id}}].data;{{"\n" if loop.last}}
    {% endfor %}
    {% if lh.args|gbl|read_or_write|length > 0 %}
    int const_bytes = 0;

        {% for arg in lh.args|gbl|read_or_write %}
            {% call opt_if(arg) %}
    const_bytes += ROUND_UP({{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_reallocConstArrays(const_bytes);
    const_bytes = 0;

        {% for arg in lh.args|gbl|read_or_write %}
            {% call opt_if(arg) %}
    args[{{arg.id}}].data   = OPP_consts_h + const_bytes;
    args[{{arg.id}}].data_d = OPP_consts_d + const_bytes;

    for (int d = 0; d < {{arg.dim}}; ++d)
        (({{arg.typ}} *)args[{{arg.id}}].data)[d] = arg{{arg.id}}_host_data[d];

    const_bytes += ROUND_UP({{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}

        {% endfor %}
    opp_mvConstArraysToDevice(const_bytes);

    {% endif %}

#ifdef OPP_BLOCK_SIZE_{{kernel_idx}}
    const int block_size = OPP_BLOCK_SIZE_{{kernel_idx}};
#else
    const int block_size = OPP_gpu_threads_per_block;
#endif

    int num_blocks = 200;

    opp_init_particle_move(set, nargs, args);
{% endblock %}

{% macro kernel_call(shared_size) %}
opp_dev_{{lh.kernel}}<<<num_blocks, block_size
{{-(", %s" % shared_size) if lh.args|gbl|reduction|length > 0 else ", 0"}}, *opp_stream>>>(
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) and lh is particle_loop %}
    ({{dat.typ}}**)args[{{dat.arg_id}}].dat->thread_data_d, array_count,    // {{dat.ptr}}
        {% else %}
    ({{dat.typ}} *)args[{{dat.arg_id}}].data_d,    // {{dat.ptr}}
        {% endif %}
    {% endfor %}
    (OPP_INT *)args[{{lh.args|length}}].data_d,    // p2c_map
    (OPP_INT *)c2c_map->map_d,    // c2c_map
    {% for map in lh.maps %}
    args[{{map.arg_id}}].map_data_d,    // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    ({{arg.typ}} *)args[{{arg.id}}].data_d,
    {% endfor %}
    (OPP_INT *)set->particle_remove_count_d,
    (OPP_INT *)OPP_remove_particle_indices_d,
    (OPP_INT *)OPP_move_particle_indices_d,
    (OPP_INT *)OPP_move_cell_indices_d,
    (OPP_INT *)OPP_move_count_d,
    {% for extra_arg in varargs %}
    {{extra_arg}}{{"," if not loop.last}}
    {% endfor %}
);
{%- endmacro %}

{% macro seg_red_kernel_call(shared_size) %}
opp_dev_sr_{{lh.kernel}}<<<num_blocks, block_size{{-(", %s" % shared_size) if lh.args|gbl|reduction|length > 0 else ", 0"}}, *opp_stream>>>( 
    {% for dat in lh.dats %}
    ({{dat.typ}} *)args[{{dat.arg_id}}].data_d,     // {{dat.ptr}}
    {% endfor %}
    (OPP_INT *)args[{{lh.args|length}}].data_d,    // p2c_map
    (OPP_INT *)c2c_map->map_d,    // c2c_map
    {% for map in lh.maps %}
    args[{{map.arg_id}}].map_data_d,     // {{map.ptr}}
    {% endfor %}
    {% for arg in lh.args|gbl %}
    ({{arg.typ}} *)args[{{arg.id}}].data_d,
    {% endfor %}
    (OPP_INT *)set->particle_remove_count_d,
    (OPP_INT *)OPP_remove_particle_indices_d,
    (OPP_INT *)OPP_move_particle_indices_d,
    (OPP_INT *)OPP_move_cell_indices_d,
    (OPP_INT *)OPP_move_count_d,
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) %}
    opp_get_dev_raw_ptr<{{dat.typ}}>(sr_dat{{dat.id}}_values_dv),     // sr values for {{dat.ptr}}
    opp_get_dev_raw_ptr<OPP_INT>(sr_dat{{dat.id}}_keys_dv),     // sr keys for {{dat.ptr}}
        {% endif %}
    {% endfor %}
    {% for extra_arg in varargs %}
    {{extra_arg}}{{"," if not loop.last}}
    {% endfor %}
);
{%- endmacro %}

{%- macro dh_check_for_global_move() %}
    if (useGlobalMove) {
           
#ifdef USE_MPI         
        globalMover->initGlobalMove();
        opp_init_dh_device(set);
#endif
        opp_profiler->start("GblMv_Move");

        opp_mem::dev_copy_to_symbol<OPP_INT>(cellMapper_pos_stride_d, &cellMapper_pos_stride, &(args[0].dat->set->set_capacity), 1);
        opp_mem::dev_copy_to_symbol<OPP_INT>(OPP_rank_d, &OPP_rank, 1);

        OPP_DEV_CHECK(hipMemcpyToSymbol(HIP_SYMBOL(opp_minSavedDHGrid_d), opp_minSavedDHGrid, 3 * sizeof(size_t)));
        OPP_DEV_CHECK(hipMemcpyToSymbol(HIP_SYMBOL(opp_maxSavedDHGrid_d), opp_maxSavedDHGrid, 3 * sizeof(size_t)));

        // check whether particles need to be moved via the global move routine
        num_blocks = (OPP_iter_end - OPP_iter_start - 1) / block_size + 1;
        opp_dev_checkForGlobalMove{{lh.dat(lh.args[0]).dim}}D_kernel<<<num_blocks, block_size, 0, *opp_stream>>>(
            (OPP_REAL*)args[0].data_d,    // {{lh.dat(lh.args[0]).ptr}} 
            (OPP_INT *)args[{{lh.args|length}}].data_d,    // p2c_map
            cellMapper->structMeshToCellMapping_d, 
            cellMapper->structMeshToRankMapping_d,
            cellMapper->oneOverGridSpacing_d, 
            cellMapper->minGlbCoordinate_d, 
            cellMapper->globalGridDims_d, 
            cellMapper->globalGridSize_d,
            set->particle_remove_count_d, 
            OPP_remove_particle_indices_d, 
            dh_indices_d.part_indices, 
            dh_indices_d.cell_indices, 
            dh_indices_d.rank_indices, 
            dh_indices_d.move_count,
            OPP_iter_start, OPP_iter_end
        );
        OPP_DEVICE_SYNCHRONIZE();

        opp_profiler->end("GblMv_Move");

#ifdef USE_MPI 
        opp_gather_dh_move_indices(set);
        globalMover->communicate(set);
#endif
    }
{%- endmacro %}

{%- macro dh_finalize() %}
#ifdef USE_MPI 
    // ----------------------------------------------------------------------------
    // finalize the global move routine and iterate over newly added particles and check whether they need neighbour comm
    if (useGlobalMove) { 
        
        opp_profiler->start("GblMv_finalize");
        const int finalized = globalMover->finalize(set);
        opp_profiler->end("GblMv_finalize");

        if (finalized > 0) {
            opp_profiler->start("GblMv_AllMv");

            // need to change arg data since particle resize in globalMover::finalize could change the pointer in dat->data 
            for (int i = 0; i < nargs; i++)
                if (args[i].argtype == OPP_ARG_DAT && args[i].dat->set->is_particle)
                    args[i].data_d = args[i].dat->data_d;

            // check whether the new particle is within cell, and if not move between cells within the MPI rank, 
            // mark for neighbour comm. Do only for the globally moved particles 
            const int start2 = (set->size - set->diff);
            const int end2 = set->size;
            num_blocks = (end2 - start2 - 1) / block_size + 1;

    {% for dat in lh.dats|soa %}
            opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d, &opp_k{{kernel_idx}}_dat{{dat.id}}_stride, &(args[{{dat.arg_id}}].dat->set->set_capacity), 1);
    {% endfor %}
    {% for map in lh.maps %}
            opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_map{{map.id}}_stride_d, &opp_k{{kernel_idx}}_map{{map.id}}_stride, &(args[{{map.arg_id}}].size), 1);
    {% endfor %}

            opp_profiler->start("move_kernel_only");      
            {{kernel_call("(reduction_size * block_size)",
                    "start2", "end2")|indent(12)}}
            OPP_DEVICE_SYNCHRONIZE(); 
            opp_profiler->end("move_kernel_only");

            opp_profiler->end("GblMv_AllMv");
        }
    }
#endif
{%- endmacro %}


{%- macro generate_storage_for_global_reductions() -%}
    {% if lh.args|gbl|reduction|length > 0 %}
        {% if lh is direct %}
    int max_blocks = num_blocks;
        {% elif config.atomics %}
    int max_blocks = (MAX(set->core_size, set->size + set->exec_size - set->core_size) - 1) / block_size + 1;
        {% endif %}

    int reduction_bytes = 0;
    int reduction_size = 0;

        {% for arg in lh.args|gbl|reduction %}
            {% call opt_if(arg) %}
    reduction_bytes += ROUND_UP(max_blocks * {{arg.dim}} * sizeof({{arg.typ}}));
    reduction_size   = MAX(reduction_size, sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_reallocReductArrays(reduction_bytes);
    reduction_bytes = 0;

        {% for arg in lh.args|gbl|reduction %}
            {% call opt_if(arg) %}
    args[{{arg.id}}].data   = OPP_reduct_h + reduction_bytes;
    args[{{arg.id}}].data_d = OPP_reduct_d + reduction_bytes;

    for (int b = 0; b < max_blocks; ++b) {
        for (int d = 0; d < {{arg.dim}}; ++d)
            (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d] = {% if arg.access_type == OP.AccessType.INC -%}
                {{arg.typ}}_ZERO
            {%- else -%}
                arg{{arg.id}}_host_data[d]
            {%- endif %};
    }

    reduction_bytes += ROUND_UP(max_blocks * {{arg.dim}} * sizeof({{arg.typ}}));
            {% endcall %}
        {% endfor %}

    opp_mvReductArraysToDevice(reduction_bytes);
    {% endif %}
{%- endmacro %}

{%- macro reduce_global_reductions() -%}
    {% if lh.args|gbl|read_write|length > 0 or lh.args|gbl|write|length > 0 %}
    mvConstArraysToHost(const_bytes);
    
        {% for arg in lh.args|gbl if arg is write or arg is read_write %}
    for (int d = 0; d < {{arg.dim}}; ++d)
        arg{{arg.id}}_host_data[d]; = (({{arg.typ}} *)args[{{arg.id}}].data)[d];
        {% endfor %}
    {% endif %}
    {% for arg in lh.args|gbl|read_or_write %}
    args[{{arg.id}}].data = (char *)arg{{arg.id}}_host_data;{{"\n" if loop.last}}
    {% endfor %}
    {% for arg in lh.args|gbl|reduction %}
    for (int b = 0; {{opt_cond_comp(arg)}}b < max_blocks; ++b) {
        for (int d = 0; d < {{arg.dim}}; ++d)
        {% if arg.access_type == OP.AccessType.INC %}
            arg{{arg.id}}_host_data[d] += (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d];
        {% elif arg.access_type in [OP.AccessType.MIN, OP.AccessType.MAX] %}
            arg{{arg.id}}_host_data[d] = {{arg.access_type.name-}}
                (arg{{arg.id}}_host_data[d], (({{arg.typ}} *)args[{{arg.id}}].data)[b * {{arg.dim}} + d]);
        {% endif %}
    }

    {% endfor %}
    {% for arg in lh.args|gbl|reduction %}
        {% call opt_if(arg) %}
    args[{{arg.id}}].data = (char *)arg{{arg.id}}_host_data;
    opp_mpi_reduce(&args[{{arg.id}}], arg{{arg.id}}_host_data);
        {% endcall %}

    {% endfor %}
{%- endmacro %}

{%- macro create_arrays_for_particle_indirect_reductions() -%}
    {% if lh.args|dat|indirect|reduction|length > 0 and lh is particle_loop %}
    const int array_count = opp_params->get<OPP_INT>("gpu_reduction_arrays");
    {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.atomics %}
    if (!opp_use_segmented_reductions) {
        {% for dat in lh.dats %}
            {% if dat is indirect_reduction(lh) and lh is particle_loop %}
        opp_create_thread_level_data<{{dat.typ}}>(args[{{dat.arg_id}}]);
            {% endif %}
        {% endfor %}
    }
    {% endif %}
{%- endmacro %}

{%- macro reduce_particle_indirect_reduction_arrays() -%}
    {% for dat in lh.dats %}
        {% if dat is indirect_reduction(lh) and lh is particle_loop %}
    opp_reduce_thread_level_data<{{dat.typ}}>(args[{{dat.id}}]);
        {% endif %}
    {% endfor %}
{%- endmacro %}

{% macro multihop_move(pre) %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(OPP_comm_iteration_d, &OPP_comm_iteration, 1);
    num_blocks = (OPP_iter_end - OPP_iter_start - 1) / block_size + 1;
    
    {% for dat in lh.dats|soa %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_dat{{dat.id}}_stride_d, &opp_k{{kernel_idx}}_dat{{dat.id}}_stride, &(args[{{dat.arg_id}}].dat->set->set_capacity), 1);
    {% endfor %}
    {% for map in lh.maps %}
    opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_map{{map.id}}_stride_d, &opp_k{{kernel_idx}}_map{{map.id}}_stride, &(args[{{map.arg_id}}].size), 1);
    {% endfor %}

    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.atomics and config.seg_red %}
    if (!opp_use_segmented_reductions) // Do atomics ----------       
    {% endif %}
    {% if lh is direct or config.atomics %}
    {
        opp_profiler->start("move_kernel_only");
        {{kernel_call("(reduction_size * block_size)",
            "OPP_iter_start", "OPP_iter_end")|indent(8)}}
        OPP_DEVICE_SYNCHRONIZE(); 
        opp_profiler->end("move_kernel_only");
    }
    {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.atomics and config.seg_red %}
    else // Do segmented reductions ----------       
    {% endif %}
    {% if lh is indirect and lh.args|dat|indirect|reduction|length > 0 and config.seg_red %}
    {
        {{pre}}opp_mem::dev_copy_to_symbol<OPP_INT>(opp_k{{kernel_idx}}_sr_set_stride_d, &opp_k{{kernel_idx}}_sr_set_stride, &set->size, 1);

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
        {{pre}}size_t operating_size_dat{{arg.dat_id}} = 0, resize_size_dat{{arg.dat_id}} = 0;
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction %}
        {{pre}}operating_size_dat{{arg.dat_id}} += (size_t)1;
        {{pre}}resize_size_dat{{arg.dat_id}} += (size_t)1;
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
        {{pre}}operating_size_dat{{arg.dat_id}} *= (size_t)(OPP_iter_end - OPP_iter_start);
        {{pre}}resize_size_dat{{arg.dat_id}} *= (size_t)(set->set_capacity);
        {% endfor %}

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
        {{pre}}opp_sr::init_arrays<{{lh.dat(arg).typ}}>(args[{{arg.id}}].dat->dim, operating_size_dat{{arg.dat_id}}, resize_size_dat{{arg.dat_id}},
        {{pre}}            sr_dat{{arg.dat_id}}_keys_dv, sr_dat{{arg.dat_id}}_values_dv, sr_dat{{arg.dat_id}}_keys_dv2, sr_dat{{arg.dat_id}}_values_dv2);
        {% endfor %}

        // Create key/value pairs
        opp_profiler->start("SRM_CrKeyVal");
        {{seg_red_kernel_call("(reduction_size * block_size)",
            "OPP_iter_start", "OPP_iter_end")|indent(8)}}
        OPP_DEVICE_SYNCHRONIZE();
        opp_profiler->end("SRM_CrKeyVal");

        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) %}
        {{pre}}opp_sr::do_segmented_reductions<{{lh.dat(arg).typ}}>(args[{{arg.dat_id}}], (OPP_iter_end - OPP_iter_start), 
        {{pre}}            sr_dat{{arg.dat_id}}_keys_dv, sr_dat{{arg.dat_id}}_values_dv, sr_dat{{arg.dat_id}}_keys_dv2, sr_dat{{arg.dat_id}}_values_dv2);
        {% endfor %}
    }
    {%- endif %}
{% endmacro %}

{% block host_loop %}
    {% if lh.dh_loop_required %}
{{ dh_check_for_global_move() }}

    {% endif%}
{{ generate_storage_for_global_reductions() }}
{{ create_arrays_for_particle_indirect_reductions() }}

    opp_profiler->start("Mv_AllMv0");
    {% if lh.dh_loop_required %}
    // ----------------------------------------------------------------------------
    // check whether all particles not marked for global comm is within cell, 
    // and if not mark to move between cells within the MPI rank, mark for neighbour comm
    {% else %}
    // ----------------------------------------------------------------------------
    // Multi-hop move particles within current MPI rank and if not mark for neighbour comm
    {% endif %}

{{ multihop_move("") }}

    opp_profiler->end("Mv_AllMv0");
    {% if lh.dh_loop_required %}    
{{ dh_finalize() }}
    {% endif%}

    // ----------------------------------------------------------------------------
    // Do neighbour communication and if atleast one particle is received by the currect rank, 
    // then iterate over the newly added particles
    while (opp_finalize_particle_move(set)) {

        opp_init_particle_move(set, nargs, args);

    {{ multihop_move("// ")|indent(4) }}
    }

{{ reduce_global_reductions() }}
    {%- if lh is indirect and lh.args|dat|indirect|reduction|length > 0 %}
    if (!opp_use_segmented_reductions) {
    {{ reduce_particle_indirect_reduction_arrays() }}
    }
    else {
        {% for arg in lh.args|dat|indirect|reduction|not_already_mapped(lh) if config.seg_red %}
        opp_sr::clear_arrays<{{lh.dat(arg).typ}}>(sr_dat{{arg.dat_id}}_keys_dv, sr_dat{{arg.dat_id}}_values_dv, sr_dat{{arg.dat_id}}_keys_dv2, sr_dat{{arg.dat_id}}_values_dv2);
        {% endfor %}
    }

    {% endif %}
{% endblock %}

{% block host_epilogue %}
    opp_set_dirtybit_grouped(nargs, args, Device_GPU);
    OPP_DEVICE_SYNCHRONIZE();   
    {% if lh is double_indirect_reduc %}

#ifdef USE_MPI    
    opp_exchange_double_indirect_reductions_device(nargs, args);
    opp_complete_double_indirect_reductions_device(nargs, args);
#endif
    {% endif %} 
{{super()}}
{% endblock %}

{% block dh_init_wrapper %}
{% if lh.dh_loop_required %}

void opp_init_direct_hop_cg(double grid_spacing, const opp_dat c_gbl_id, const opp::BoundingBox& b_box, 
    opp_map c2c_map, opp_map p2c_map,
    {% for arg in lh.args %}
    opp_arg arg{{arg.id}}{{"," if not loop.last}} // {% if arg is dat %}{{lh.dat(arg).ptr}} {% endif -%} | OPP_{{arg.access_type.name}}
    {% endfor %}
) { OPP_RETURN_IF_INVALID_PROCESS;

    opp_profiler->start("Setup_Mover");

    useGlobalMove = opp_params->get<OPP_BOOL>("opp_global_move");

    if (OPP_DBG) opp_printf("opp_init_direct_hop_cg", "START useGlobalMove=%s", useGlobalMove ? "YES" : "NO");

    if (useGlobalMove) {

        const int nargs = {{lh.args|length + 1}};
        opp_arg args[nargs];

    {% for arg in lh.args %}
        args[{{loop.index0}}] = {{arg_dat_redef(arg) if lh.args[arg.id] is vec else "arg%d" % arg.id}};
    {% endfor %}
        args[{{lh.args|length}}] = opp_arg_dat(p2c_map->p2c_dat, OPP_RW); // required to make dirty or should manually make it dirty

#ifdef USE_MPI
        opp_mpi_halo_exchanges_grouped(c_gbl_id->set, nargs, args, Device_CPU);

        comm = std::make_shared<opp::Comm>(OPP_MPI_WORLD);
        globalMover = std::make_unique<opp::GlobalParticleMover>(comm->comm_parent);

        opp_mpi_halo_wait_all(nargs, args);
#endif

        boundingBox = std::make_shared<opp::BoundingBox>(b_box);
        cellMapper = std::make_shared<opp::CellMapper>(boundingBox, grid_spacing, comm);
        
        const int c_set_size = c_gbl_id->set->size;

        // lambda function for dh mesh search loop
        auto all_cell_checker = [&](const opp_point& point, int& cid) {          
    {% for arg in lh.args|dat %}
        {% if arg.access_type.value in [1, 3, 4, 5, 2] %} 
            // we dont want to change the original arrays during dh mesh generation, hence duplicate except OPP_READ
            {{lh.dat(arg).typ}} arg{{arg.id}}_temp[{{lh.dat(arg).dim}}];
        {%- endif %}
    {%- endfor +%}
            for (int ci = 0; ci < c_set_size; ++ci) {
                opp_move_status_flag = OPP_NEED_MOVE;  
                opp_move_hop_iter_one_flag = true;
                
                int temp_ci = ci; // we dont want to get iterating ci changed within the kernel, hence get a copy
                
                opp_p2c = &(temp_ci);           
                opp_c2c = &((c2c_map->map)[temp_ci * {{lh.c2c_map.dim}}]);
    {% if lh is indirect %}
        {% for map in lh.maps %}
                const OPP_INT *map{{map.id}} = args[{{map.arg_id}}].map_data + (temp_ci * {{map.dim}});
        {%- endfor %}   
    {%- endif %}    
    {%- for arg in lh.args|dat %}
        {% if arg.access_type.value == 2 %} 

            {%- if arg is double_indirect -%}
            {%- set offset = " + (map%s[%d] * %d)" % (arg.map_id, arg.map_idx, lh.dat(arg).dim) -%}
            {%- elif arg is indirect -%}
            {%- set offset = " + (temp_ci * %d)" % (lh.dat(arg).dim) -%}
            {%- endif %}
                // arg{{arg.id}} is OPP_RW, hence get a copy just incase
                std::memcpy(&arg{{arg.id}}_temp, ({{lh.dat(arg).typ}} *)args[{{arg.id}}].data{{offset}}, (sizeof({{lh.dat(arg).typ}}) * {{lh.dat(arg).dim}}));
        {% endif %}
    {%- endfor %}

                opp_k{{kernel_idx}}::host::{{lh.kernel}}(
    {% for arg in lh.args %}
                    {{arg_to_pointer_dh(arg,lh)}}
    {% endfor %}
                );
                if (opp_move_status_flag == OPP_MOVE_DONE) {       
                    cid = temp_ci;
                    break;
                }
            }
        };

        if (opp_params->get<OPP_BOOL>("opp_dh_data_generate")) {
            cellMapper->generateStructuredMesh(c_gbl_id->set, c_gbl_id, all_cell_checker);
        }
        else {
            cellMapper->generateStructuredMeshFromFile(c_gbl_id->set, c_gbl_id);  
        }
    }

    opp_profiler->end("Setup_Mover");
}

{% endif %}
{% endblock %}